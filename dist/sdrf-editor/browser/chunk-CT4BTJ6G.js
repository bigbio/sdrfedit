import{d as r,k as w}from"./chunk-OYZGYKHQ.js";import{f as l,g as m,h as f}from"./chunk-KAWQVD33.js";var h=class extends w{name="ollama";supportsStreaming=!0;constructor(e={}){super(e)}getProviderType(){return"ollama"}isConfigured(){return!0}complete(e){return l(this,null,function*(){let o=`${this.config.baseUrl}/api/chat`,n={model:this.config.model,messages:this.convertMessages(e),stream:!1,options:{temperature:this.config.temperature,num_predict:this.config.maxTokens}};try{let t=yield this.fetchWithTimeout(o,{method:"POST",headers:this.getHeaders(),body:JSON.stringify(n)});if(!t.ok){let d=yield t.text();throw this.createErrorFromResponse(t.status,d)}let s=yield t.json();return this.convertResponse(s)}catch(t){throw t instanceof TypeError&&t.message.includes("fetch")?typeof window<"u"&&!window.location.hostname.includes("localhost")&&!window.location.hostname.includes("127.0.0.1")?new r(`Cannot connect to Ollama at ${this.config.baseUrl}. This is likely a CORS issue - Ollama running on localhost cannot be accessed from this remote site. Either: (1) Run the editor locally, (2) Configure Ollama with OLLAMA_ORIGINS=${window.location.origin}, or (3) Use a cloud AI provider like OpenAI or Anthropic.`,"NETWORK_ERROR","ollama"):new r("Cannot connect to Ollama. Make sure Ollama is running on "+this.config.baseUrl,"NETWORK_ERROR","ollama"):t}})}stream(e){return f(this,null,function*(){let o=`${this.config.baseUrl}/api/chat`,n={model:this.config.model,messages:this.convertMessages(e),stream:!0,options:{temperature:this.config.temperature,num_predict:this.config.maxTokens}};this.abortController=new AbortController;try{let t=yield new m(fetch(o,{method:"POST",headers:this.getHeaders(),body:JSON.stringify(n),signal:this.abortController.signal}));if(!t.ok){let c=yield new m(t.text());throw this.createErrorFromResponse(t.status,c)}if(!t.body)throw new Error("Response body is empty");let s=t.body.getReader(),d=new TextDecoder,a="";for(;;){let{done:c,value:R}=yield new m(s.read());if(c)break;a+=d.decode(R,{stream:!0});let g=a.split(`
`);a=g.pop()||"";for(let O of g){let u=O.trim();if(u)try{let i=JSON.parse(u);yield{content:i.message?.content||"",done:i.done,finishReason:i.done?"stop":void 0}}catch(i){console.warn("Failed to parse Ollama stream chunk:",u,i)}}}if(a.trim())try{yield{content:JSON.parse(a.trim()).message?.content||"",done:!0,finishReason:"stop"}}catch{}}catch(t){throw t instanceof TypeError&&t.message.includes("fetch")?typeof window<"u"&&!window.location.hostname.includes("localhost")&&!window.location.hostname.includes("127.0.0.1")?new r(`Cannot connect to Ollama at ${this.config.baseUrl}. This is likely a CORS issue - Ollama running on localhost cannot be accessed from this remote site. Either: (1) Run the editor locally, (2) Configure Ollama with OLLAMA_ORIGINS=${window.location.origin}, or (3) Use a cloud AI provider like OpenAI or Anthropic.`,"NETWORK_ERROR","ollama"):new r("Cannot connect to Ollama. Make sure Ollama is running.","NETWORK_ERROR","ollama"):t instanceof Error&&t.name==="AbortError"?new r("Request was cancelled","ABORTED","ollama"):t}})}listModels(){return l(this,null,function*(){try{let e=yield fetch(`${this.config.baseUrl}/api/tags`,{headers:this.getHeaders()});return e.ok?((yield e.json()).models||[]).map(n=>n.name):[]}catch{return[]}})}isRunning(){return l(this,null,function*(){try{return(yield fetch(`${this.config.baseUrl}/api/tags`,{signal:AbortSignal.timeout(5e3)})).ok}catch{return!1}})}pullModel(e){return l(this,null,function*(){let o=yield fetch(`${this.config.baseUrl}/api/pull`,{method:"POST",headers:this.getHeaders(),body:JSON.stringify({name:e,stream:!1})});if(!o.ok){let n=yield o.text();throw new r(`Failed to pull model ${e}: ${n}`,"PROVIDER_ERROR","ollama")}})}getHeaders(){return{"Content-Type":"application/json"}}convertMessages(e){return e.map(o=>({role:o.role,content:o.content}))}convertResponse(e){let o=e.prompt_eval_count!==void 0||e.eval_count!==void 0?{promptTokens:e.prompt_eval_count||0,completionTokens:e.eval_count||0,totalTokens:(e.prompt_eval_count||0)+(e.eval_count||0)}:void 0;return{content:e.message?.content||"",finishReason:e.done?"stop":void 0,usage:o,raw:e}}};function E(p){return new h(p)}export{h as OllamaProvider,E as createOllamaProvider};
