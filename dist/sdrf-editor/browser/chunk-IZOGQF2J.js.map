{
  "version": 3,
  "sources": ["src/app/core/services/llm/providers/ollama-provider.ts"],
  "sourcesContent": ["/**\n * Ollama LLM Provider\n *\n * Implements the Ollama API for local LLM inference.\n * https://github.com/ollama/ollama/blob/main/docs/api.md\n */\n\nimport {\n  LlmProviderType,\n  LlmProviderConfig,\n  LlmMessage,\n  LlmResponse,\n  LlmStreamChunk,\n  LlmUsage,\n  LlmError,\n} from '../../../models/llm';\nimport { BaseLlmProvider } from './base-provider';\n\n/**\n * Ollama message format.\n */\ninterface OllamaMessage {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n}\n\n/**\n * Ollama chat request body.\n */\ninterface OllamaChatRequest {\n  model: string;\n  messages: OllamaMessage[];\n  stream?: boolean;\n  options?: {\n    temperature?: number;\n    num_predict?: number;\n  };\n}\n\n/**\n * Ollama chat response.\n */\ninterface OllamaChatResponse {\n  model: string;\n  created_at: string;\n  message: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  prompt_eval_duration?: number;\n  eval_count?: number;\n  eval_duration?: number;\n}\n\n/**\n * Ollama streaming response chunk.\n */\ninterface OllamaStreamChunk {\n  model: string;\n  created_at: string;\n  message: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  total_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n}\n\n/**\n * Ollama LLM Provider implementation.\n */\nexport class OllamaProvider extends BaseLlmProvider {\n  readonly name: LlmProviderType = 'ollama';\n  readonly supportsStreaming = true;\n\n  constructor(config: Partial<LlmProviderConfig> = {}) {\n    super(config);\n  }\n\n  protected getProviderType(): LlmProviderType {\n    return 'ollama';\n  }\n\n  /**\n   * Ollama doesn't require an API key.\n   */\n  override isConfigured(): boolean {\n    return true;\n  }\n\n  /**\n   * Sends a completion request to Ollama.\n   */\n  async complete(messages: LlmMessage[]): Promise<LlmResponse> {\n    const url = `${this.config.baseUrl}/api/chat`;\n    const body: OllamaChatRequest = {\n      model: this.config.model,\n      messages: this.convertMessages(messages),\n      stream: false,\n      options: {\n        temperature: this.config.temperature,\n        num_predict: this.config.maxTokens,\n      },\n    };\n\n    try {\n      const response = await this.fetchWithTimeout(url, {\n        method: 'POST',\n        headers: this.getHeaders(),\n        body: JSON.stringify(body),\n      });\n\n      if (!response.ok) {\n        const errorBody = await response.text();\n        throw this.createErrorFromResponse(response.status, errorBody);\n      }\n\n      const data: OllamaChatResponse = await response.json();\n      return this.convertResponse(data);\n    } catch (error) {\n      // Check if Ollama is running\n      if (error instanceof TypeError && error.message.includes('fetch')) {\n        throw new LlmError(\n          'Cannot connect to Ollama. Make sure Ollama is running on ' + this.config.baseUrl,\n          'NETWORK_ERROR',\n          'ollama'\n        );\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Streams a completion from Ollama.\n   * Note: Ollama can be slow on first request while loading the model.\n   */\n  async *stream(messages: LlmMessage[]): AsyncGenerator<LlmStreamChunk, void, unknown> {\n    const url = `${this.config.baseUrl}/api/chat`;\n    const body: OllamaChatRequest = {\n      model: this.config.model,\n      messages: this.convertMessages(messages),\n      stream: true,\n      options: {\n        temperature: this.config.temperature,\n        num_predict: this.config.maxTokens,\n      },\n    };\n\n    // Create new abort controller for this request\n    this.abortController = new AbortController();\n\n    try {\n      const response = await fetch(url, {\n        method: 'POST',\n        headers: this.getHeaders(),\n        body: JSON.stringify(body),\n        signal: this.abortController.signal,\n      });\n\n      if (!response.ok) {\n        const errorBody = await response.text();\n        throw this.createErrorFromResponse(response.status, errorBody);\n      }\n\n      if (!response.body) {\n        throw new Error('Response body is empty');\n      }\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done) {\n          break;\n        }\n\n        buffer += decoder.decode(value, { stream: true });\n\n        // Ollama returns newline-delimited JSON\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          const trimmed = line.trim();\n          if (!trimmed) continue;\n\n          try {\n            const chunk: OllamaStreamChunk = JSON.parse(trimmed);\n\n            yield {\n              content: chunk.message?.content || '',\n              done: chunk.done,\n              finishReason: chunk.done ? 'stop' : undefined,\n            };\n          } catch (e) {\n            console.warn('Failed to parse Ollama stream chunk:', trimmed, e);\n          }\n        }\n      }\n\n      // Process any remaining buffer\n      if (buffer.trim()) {\n        try {\n          const chunk: OllamaStreamChunk = JSON.parse(buffer.trim());\n          yield {\n            content: chunk.message?.content || '',\n            done: true,\n            finishReason: 'stop',\n          };\n        } catch {\n          // Ignore final buffer parse errors\n        }\n      }\n    } catch (error) {\n      if (error instanceof TypeError && error.message.includes('fetch')) {\n        throw new LlmError(\n          'Cannot connect to Ollama. Make sure Ollama is running.',\n          'NETWORK_ERROR',\n          'ollama'\n        );\n      }\n      // Handle abort errors gracefully\n      if (error instanceof Error && error.name === 'AbortError') {\n        throw new LlmError(\n          'Request was cancelled',\n          'ABORTED',\n          'ollama'\n        );\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Lists available models from Ollama.\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.config.baseUrl}/api/tags`, {\n        headers: this.getHeaders(),\n      });\n\n      if (!response.ok) {\n        return [];\n      }\n\n      const data = await response.json();\n      return (data.models || []).map((m: any) => m.name);\n    } catch {\n      return [];\n    }\n  }\n\n  /**\n   * Checks if Ollama is running and accessible.\n   */\n  async isRunning(): Promise<boolean> {\n    try {\n      const response = await fetch(`${this.config.baseUrl}/api/tags`, {\n        signal: AbortSignal.timeout(5000),\n      });\n      return response.ok;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Pulls a model if not already available.\n   */\n  async pullModel(modelName: string): Promise<void> {\n    const response = await fetch(`${this.config.baseUrl}/api/pull`, {\n      method: 'POST',\n      headers: this.getHeaders(),\n      body: JSON.stringify({ name: modelName, stream: false }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new LlmError(\n        `Failed to pull model ${modelName}: ${error}`,\n        'PROVIDER_ERROR',\n        'ollama'\n      );\n    }\n  }\n\n  // ============ Private Methods ============\n\n  /**\n   * Gets request headers.\n   */\n  private getHeaders(): Record<string, string> {\n    return {\n      'Content-Type': 'application/json',\n    };\n  }\n\n  /**\n   * Converts our message format to Ollama format.\n   */\n  private convertMessages(messages: LlmMessage[]): OllamaMessage[] {\n    return messages.map((msg) => ({\n      role: msg.role,\n      content: msg.content,\n    }));\n  }\n\n  /**\n   * Converts Ollama response to our format.\n   */\n  private convertResponse(data: OllamaChatResponse): LlmResponse {\n    const usage: LlmUsage | undefined =\n      data.prompt_eval_count !== undefined || data.eval_count !== undefined\n        ? {\n            promptTokens: data.prompt_eval_count || 0,\n            completionTokens: data.eval_count || 0,\n            totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0),\n          }\n        : undefined;\n\n    return {\n      content: data.message?.content || '',\n      finishReason: data.done ? 'stop' : undefined,\n      usage,\n      raw: data,\n    };\n  }\n}\n\n/**\n * Factory function to create an Ollama provider.\n */\nexport function createOllamaProvider(\n  config?: Partial<LlmProviderConfig>\n): OllamaProvider {\n  return new OllamaProvider(config);\n}\n"],
  "mappings": ";;;;;;;;;;;AA6EM,IAAO,iBAAP,cAA8B,gBAAe;EACxC,OAAwB;EACxB,oBAAoB;EAE7B,YAAY,SAAqC,CAAA,GAAE;AACjD,UAAM,MAAM;EACd;EAEU,kBAAe;AACvB,WAAO;EACT;;;;EAKS,eAAY;AACnB,WAAO;EACT;;;;EAKM,SAAS,UAAsB;;AACnC,YAAM,MAAM,GAAG,KAAK,OAAO,OAAO;AAClC,YAAM,OAA0B;QAC9B,OAAO,KAAK,OAAO;QACnB,UAAU,KAAK,gBAAgB,QAAQ;QACvC,QAAQ;QACR,SAAS;UACP,aAAa,KAAK,OAAO;UACzB,aAAa,KAAK,OAAO;;;AAI7B,UAAI;AACF,cAAM,WAAW,MAAM,KAAK,iBAAiB,KAAK;UAChD,QAAQ;UACR,SAAS,KAAK,WAAU;UACxB,MAAM,KAAK,UAAU,IAAI;SAC1B;AAED,YAAI,CAAC,SAAS,IAAI;AAChB,gBAAM,YAAY,MAAM,SAAS,KAAI;AACrC,gBAAM,KAAK,wBAAwB,SAAS,QAAQ,SAAS;QAC/D;AAEA,cAAM,OAA2B,MAAM,SAAS,KAAI;AACpD,eAAO,KAAK,gBAAgB,IAAI;MAClC,SAAS,OAAO;AAEd,YAAI,iBAAiB,aAAa,MAAM,QAAQ,SAAS,OAAO,GAAG;AACjE,gBAAM,IAAI,SACR,8DAA8D,KAAK,OAAO,SAC1E,iBACA,QAAQ;QAEZ;AACA,cAAM;MACR;IACF;;;;;;EAMO,OAAO,UAAsB;;AAClC,YAAM,MAAM,GAAG,KAAK,OAAO,OAAO;AAClC,YAAM,OAA0B;QAC9B,OAAO,KAAK,OAAO;QACnB,UAAU,KAAK,gBAAgB,QAAQ;QACvC,QAAQ;QACR,SAAS;UACP,aAAa,KAAK,OAAO;UACzB,aAAa,KAAK,OAAO;;;AAK7B,WAAK,kBAAkB,IAAI,gBAAe;AAE1C,UAAI;AACF,cAAM,WAAW,kBAAM,MAAM,KAAK;UAChC,QAAQ;UACR,SAAS,KAAK,WAAU;UACxB,MAAM,KAAK,UAAU,IAAI;UACzB,QAAQ,KAAK,gBAAgB;SAC9B;AAED,YAAI,CAAC,SAAS,IAAI;AAChB,gBAAM,YAAY,kBAAM,SAAS,KAAI;AACrC,gBAAM,KAAK,wBAAwB,SAAS,QAAQ,SAAS;QAC/D;AAEA,YAAI,CAAC,SAAS,MAAM;AAClB,gBAAM,IAAI,MAAM,wBAAwB;QAC1C;AAEA,cAAM,SAAS,SAAS,KAAK,UAAS;AACtC,cAAM,UAAU,IAAI,YAAW;AAC/B,YAAI,SAAS;AAEb,eAAO,MAAM;AACX,gBAAM,EAAE,MAAM,MAAK,IAAK,kBAAM,OAAO,KAAI;AAEzC,cAAI,MAAM;AACR;UACF;AAEA,oBAAU,QAAQ,OAAO,OAAO,EAAE,QAAQ,KAAI,CAAE;AAGhD,gBAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,mBAAS,MAAM,IAAG,KAAM;AAExB,qBAAW,QAAQ,OAAO;AACxB,kBAAM,UAAU,KAAK,KAAI;AACzB,gBAAI,CAAC;AAAS;AAEd,gBAAI;AACF,oBAAM,QAA2B,KAAK,MAAM,OAAO;AAEnD,oBAAM;gBACJ,SAAS,MAAM,SAAS,WAAW;gBACnC,MAAM,MAAM;gBACZ,cAAc,MAAM,OAAO,SAAS;;YAExC,SAAS,GAAG;AACV,sBAAQ,KAAK,wCAAwC,SAAS,CAAC;YACjE;UACF;QACF;AAGA,YAAI,OAAO,KAAI,GAAI;AACjB,cAAI;AACF,kBAAM,QAA2B,KAAK,MAAM,OAAO,KAAI,CAAE;AACzD,kBAAM;cACJ,SAAS,MAAM,SAAS,WAAW;cACnC,MAAM;cACN,cAAc;;UAElB,QAAQ;UAER;QACF;MACF,SAAS,OAAO;AACd,YAAI,iBAAiB,aAAa,MAAM,QAAQ,SAAS,OAAO,GAAG;AACjE,gBAAM,IAAI,SACR,0DACA,iBACA,QAAQ;QAEZ;AAEA,YAAI,iBAAiB,SAAS,MAAM,SAAS,cAAc;AACzD,gBAAM,IAAI,SACR,yBACA,WACA,QAAQ;QAEZ;AACA,cAAM;MACR;IACF;;;;;EAKM,aAAU;;AACd,UAAI;AACF,cAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,OAAO,aAAa;UAC9D,SAAS,KAAK,WAAU;SACzB;AAED,YAAI,CAAC,SAAS,IAAI;AAChB,iBAAO,CAAA;QACT;AAEA,cAAM,OAAO,MAAM,SAAS,KAAI;AAChC,gBAAQ,KAAK,UAAU,CAAA,GAAI,IAAI,CAAC,MAAW,EAAE,IAAI;MACnD,QAAQ;AACN,eAAO,CAAA;MACT;IACF;;;;;EAKM,YAAS;;AACb,UAAI;AACF,cAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,OAAO,aAAa;UAC9D,QAAQ,YAAY,QAAQ,GAAI;SACjC;AACD,eAAO,SAAS;MAClB,QAAQ;AACN,eAAO;MACT;IACF;;;;;EAKM,UAAU,WAAiB;;AAC/B,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,OAAO,aAAa;QAC9D,QAAQ;QACR,SAAS,KAAK,WAAU;QACxB,MAAM,KAAK,UAAU,EAAE,MAAM,WAAW,QAAQ,MAAK,CAAE;OACxD;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,QAAQ,MAAM,SAAS,KAAI;AACjC,cAAM,IAAI,SACR,wBAAwB,SAAS,KAAK,KAAK,IAC3C,kBACA,QAAQ;MAEZ;IACF;;;;;;EAOQ,aAAU;AAChB,WAAO;MACL,gBAAgB;;EAEpB;;;;EAKQ,gBAAgB,UAAsB;AAC5C,WAAO,SAAS,IAAI,CAAC,SAAS;MAC5B,MAAM,IAAI;MACV,SAAS,IAAI;MACb;EACJ;;;;EAKQ,gBAAgB,MAAwB;AAC9C,UAAM,QACJ,KAAK,sBAAsB,UAAa,KAAK,eAAe,SACxD;MACE,cAAc,KAAK,qBAAqB;MACxC,kBAAkB,KAAK,cAAc;MACrC,cAAc,KAAK,qBAAqB,MAAM,KAAK,cAAc;QAEnE;AAEN,WAAO;MACL,SAAS,KAAK,SAAS,WAAW;MAClC,cAAc,KAAK,OAAO,SAAS;MACnC;MACA,KAAK;;EAET;;AAMI,SAAU,qBACd,QAAmC;AAEnC,SAAO,IAAI,eAAe,MAAM;AAClC;",
  "names": []
}
