{
  "version": 3,
  "sources": ["src/app/core/models/llm.ts", "src/app/core/services/llm/providers/base-provider.ts"],
  "sourcesContent": ["/**\n * LLM Models and Types\n *\n * Types and interfaces for the LLM-based recommendation system.\n * Supports multiple providers: OpenAI, Anthropic, Gemini, and Ollama.\n */\n\n// ============ Provider Types ============\n\n/**\n * Supported LLM providers.\n */\nexport type LlmProviderType = 'openai' | 'anthropic' | 'gemini' | 'ollama';\n\n/**\n * LLM provider configuration.\n */\nexport interface LlmProviderConfig {\n  /** Provider type */\n  provider: LlmProviderType;\n\n  /** API key (not needed for Ollama) */\n  apiKey?: string;\n\n  /** Model name/ID */\n  model: string;\n\n  /** Base URL (for Ollama or custom endpoints) */\n  baseUrl?: string;\n\n  /** Request timeout in milliseconds */\n  timeoutMs?: number;\n\n  /** Maximum tokens to generate */\n  maxTokens?: number;\n\n  /** Temperature for response generation (0-1) */\n  temperature?: number;\n}\n\n/**\n * Default configurations per provider.\n */\nexport const DEFAULT_PROVIDER_CONFIGS: Record<LlmProviderType, Partial<LlmProviderConfig>> = {\n  openai: {\n    model: 'gpt-4o-mini',\n    baseUrl: 'https://api.openai.com/v1',\n    timeoutMs: 60000,\n    maxTokens: 4096,\n    temperature: 0.3,\n  },\n  anthropic: {\n    model: 'claude-sonnet-4-20250514',\n    baseUrl: 'https://api.anthropic.com/v1',\n    timeoutMs: 60000,\n    maxTokens: 4096,\n    temperature: 0.3,\n  },\n  gemini: {\n    model: 'gemini-2.0-flash',\n    baseUrl: 'https://generativelanguage.googleapis.com/v1beta',\n    timeoutMs: 60000,\n    maxTokens: 4096,\n    temperature: 0.3,\n  },\n  ollama: {\n    model: 'qwen3',\n    baseUrl: 'http://localhost:11434',\n    timeoutMs: 120000,\n    maxTokens: 4096,\n    temperature: 0.3,\n  },\n};\n\n/**\n * Available models per provider.\n */\nexport const AVAILABLE_MODELS: Record<LlmProviderType, string[]> = {\n  openai: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo'],\n  anthropic: ['claude-sonnet-4-20250514', 'claude-3-5-haiku-20241022', 'claude-3-opus-20240229'],\n  gemini: ['gemini-2.0-flash', 'gemini-1.5-pro', 'gemini-1.5-flash'],\n  ollama: ['qwen3', 'llama3.2', 'llama3.1', 'mistral', 'mixtral', 'codellama', 'phi3'],\n};\n\n// ============ Message Types ============\n\n/**\n * Message role in a conversation.\n */\nexport type LlmMessageRole = 'system' | 'user' | 'assistant';\n\n/**\n * A message in the LLM conversation.\n */\nexport interface LlmMessage {\n  role: LlmMessageRole;\n  content: string;\n}\n\n/**\n * Response from an LLM completion.\n */\nexport interface LlmResponse {\n  /** The generated content */\n  content: string;\n\n  /** Finish reason */\n  finishReason?: 'stop' | 'length' | 'content_filter' | 'error';\n\n  /** Usage statistics */\n  usage?: LlmUsage;\n\n  /** Raw response for debugging */\n  raw?: unknown;\n}\n\n/**\n * Token usage statistics.\n */\nexport interface LlmUsage {\n  promptTokens: number;\n  completionTokens: number;\n  totalTokens: number;\n}\n\n/**\n * Streaming chunk from LLM.\n */\nexport interface LlmStreamChunk {\n  /** Delta content */\n  content: string;\n\n  /** Whether this is the final chunk */\n  done: boolean;\n\n  /** Finish reason (only on final chunk) */\n  finishReason?: string;\n}\n\n// ============ Recommendation Types ============\n\n/**\n * Types of SDRF recommendations.\n */\nexport type RecommendationType =\n  | 'fill_value'        // Fill \"not available\" or empty values\n  | 'correct_value'     // Correct invalid or inconsistent values\n  | 'ontology_suggestion' // Suggest proper ontology terms\n  | 'consistency_fix'   // Fix consistency issues across samples\n  | 'add_column';       // Add a missing column\n\n/**\n * Confidence level for a recommendation.\n */\nexport type RecommendationConfidence = 'high' | 'medium' | 'low';\n\n/**\n * A recommendation from the LLM for improving an SDRF file.\n */\nexport interface SdrfRecommendation {\n  /** Unique identifier */\n  id: string;\n\n  /** Type of recommendation */\n  type: RecommendationType;\n\n  /** Column name this recommendation applies to */\n  column: string;\n\n  /** Column index in the table */\n  columnIndex: number;\n\n  /** Sample indices this recommendation applies to (1-based) */\n  sampleIndices: number[];\n\n  /** Current value(s) in the cells */\n  currentValue?: string;\n\n  /** Suggested new value */\n  suggestedValue: string;\n\n  /** Confidence level */\n  confidence: RecommendationConfidence;\n\n  /** Human-readable explanation */\n  reasoning: string;\n\n  /** Whether this recommendation has been applied */\n  applied?: boolean;\n\n  /** Ontology ID if this is an ontology suggestion */\n  ontologyId?: string;\n\n  /** Ontology term label */\n  ontologyLabel?: string;\n}\n\n/**\n * Result from the recommendation analysis.\n */\nexport interface RecommendationResult {\n  /** List of recommendations */\n  recommendations: SdrfRecommendation[];\n\n  /** Summary statistics */\n  summary: RecommendationSummary;\n\n  /** Raw LLM response for debugging */\n  rawResponse?: string;\n\n  /** Timestamp of analysis */\n  timestamp: Date;\n\n  /** Provider used for analysis */\n  provider: LlmProviderType;\n\n  /** Model used for analysis */\n  model: string;\n}\n\n/**\n * Summary of recommendations by type.\n */\nexport interface RecommendationSummary {\n  total: number;\n  byType: Record<RecommendationType, number>;\n  byConfidence: Record<RecommendationConfidence, number>;\n  affectedColumns: string[];\n  affectedSamples: number;\n}\n\n// ============ Analysis Context Types ============\n\n/**\n * Context information sent to the LLM for analysis.\n */\nexport interface SdrfAnalysisContext {\n  /** Table metadata */\n  metadata: {\n    sampleCount: number;\n    columnCount: number;\n    columnNames: string[];\n  };\n\n  /** Column definitions with validation rules */\n  columns: ColumnContext[];\n\n  /** Issues identified for analysis */\n  issues: AnalysisIssue[];\n\n  /** Sample data (subset for context) */\n  sampleData?: string[][];\n\n  /** Focus areas for analysis */\n  focusAreas: AnalysisFocusArea[];\n}\n\n/**\n * Context for a single column.\n */\nexport interface ColumnContext {\n  name: string;\n  index: number;\n  type: string;\n  isRequired: boolean;\n  ontologies?: string[];\n  pattern?: string;\n  examples?: string[];\n  allowNotAvailable: boolean;\n  allowNotApplicable: boolean;\n  uniqueValues: string[];\n  notAvailableCount: number;\n  emptyCount: number;\n}\n\n/**\n * An issue identified for LLM analysis.\n */\nexport interface AnalysisIssue {\n  type: 'missing_value' | 'invalid_ontology' | 'pattern_mismatch' | 'inconsistency';\n  column: string;\n  columnIndex: number;\n  sampleIndices: number[];\n  currentValue?: string;\n  details?: string;\n}\n\n/**\n * Focus area for analysis.\n */\nexport type AnalysisFocusArea =\n  | 'fill_missing'      // Fill \"not available\" and empty values\n  | 'validate_ontology' // Validate and suggest ontology terms\n  | 'check_consistency' // Check data consistency\n  | 'all';              // All of the above\n\n// ============ Settings Types ============\n\n/**\n * LLM settings stored in localStorage.\n */\nexport interface LlmSettings {\n  /** Currently selected provider */\n  activeProvider: LlmProviderType;\n\n  /** Provider-specific configurations */\n  providers: Partial<Record<LlmProviderType, LlmProviderConfig>>;\n\n  /** Whether user has consented to API key storage */\n  storageConsent: boolean;\n\n  /** Storage mode for API keys */\n  storageMode: 'persistent' | 'session';\n\n  /** Last used timestamp */\n  lastUsed?: number;\n}\n\n/**\n * Default LLM settings.\n */\nexport const DEFAULT_LLM_SETTINGS: LlmSettings = {\n  activeProvider: 'openai',\n  providers: {},\n  storageConsent: false,\n  storageMode: 'session',\n};\n\n// ============ Error Types ============\n\n/**\n * LLM-specific error.\n */\nexport class LlmError extends Error {\n  constructor(\n    message: string,\n    public readonly code: LlmErrorCode,\n    public readonly provider?: LlmProviderType,\n    public readonly details?: unknown\n  ) {\n    super(message);\n    this.name = 'LlmError';\n  }\n}\n\n/**\n * Error codes for LLM operations.\n */\nexport type LlmErrorCode =\n  | 'NOT_CONFIGURED'\n  | 'INVALID_API_KEY'\n  | 'RATE_LIMITED'\n  | 'TIMEOUT'\n  | 'NETWORK_ERROR'\n  | 'PARSE_ERROR'\n  | 'PROVIDER_ERROR'\n  | 'ABORTED';\n\n// ============ Utility Functions ============\n\n/**\n * Creates an empty recommendation result.\n */\nexport function createEmptyRecommendationResult(\n  provider: LlmProviderType,\n  model: string\n): RecommendationResult {\n  return {\n    recommendations: [],\n    summary: {\n      total: 0,\n      byType: {\n        fill_value: 0,\n        correct_value: 0,\n        ontology_suggestion: 0,\n        consistency_fix: 0,\n        add_column: 0,\n      },\n      byConfidence: {\n        high: 0,\n        medium: 0,\n        low: 0,\n      },\n      affectedColumns: [],\n      affectedSamples: 0,\n    },\n    timestamp: new Date(),\n    provider,\n    model,\n  };\n}\n\n/**\n * Generates a unique recommendation ID.\n */\nexport function generateRecommendationId(): string {\n  return `rec_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;\n}\n\n/**\n * Gets display name for a provider.\n */\nexport function getProviderDisplayName(provider: LlmProviderType): string {\n  const names: Record<LlmProviderType, string> = {\n    openai: 'OpenAI',\n    anthropic: 'Claude',\n    gemini: 'Google Gemini',\n    ollama: 'Ollama (Local)',\n  };\n  return names[provider];\n}\n\n/**\n * Checks if a provider requires an API key.\n */\nexport function providerRequiresApiKey(provider: LlmProviderType): boolean {\n  return provider !== 'ollama';\n}\n\n/**\n * Validates provider configuration.\n */\nexport function isProviderConfigured(config: LlmProviderConfig | undefined): boolean {\n  if (!config) return false;\n  if (config.provider === 'ollama') return true;\n  return !!config.apiKey && config.apiKey.length > 0;\n}\n\n// ============ Chat Suggestion Types ============\n\n/**\n * Types of chat suggestions (actionable recommendations from chat).\n */\nexport type ChatSuggestionType =\n  | 'set_value'      // Set a value in one or more cells\n  | 'remove_column'  // Remove a column\n  | 'rename_column'  // Rename a column\n  | 'add_column';    // Add a new column\n\n/**\n * A suggestion from chat that can be applied to the table.\n */\nexport interface ChatSuggestion {\n  /** Unique identifier */\n  id: string;\n\n  /** Type of suggestion */\n  type: ChatSuggestionType;\n\n  /** Column name this suggestion applies to */\n  column: string;\n\n  /** Sample indices for set_value type (1-based) */\n  sampleIndices?: number[];\n\n  /** Current value (for display) */\n  currentValue?: string;\n\n  /** Suggested value */\n  suggestedValue?: string;\n\n  /** New column name (for rename_column) */\n  newColumnName?: string;\n\n  /** Human-readable description */\n  description: string;\n\n  /** Confidence level */\n  confidence: RecommendationConfidence;\n\n  /** Whether this suggestion has been applied */\n  applied?: boolean;\n\n  /** Whether this suggestion has been dismissed */\n  dismissed?: boolean;\n}\n\n/**\n * A chat message with optional suggestions.\n */\nexport interface ChatMessage {\n  /** Message role */\n  role: 'user' | 'assistant';\n\n  /** Text content */\n  content: string;\n\n  /** Actionable suggestions (assistant messages only) */\n  suggestions?: ChatSuggestion[];\n\n  /** Timestamp */\n  timestamp?: Date;\n}\n\n/**\n * Parsed chat response from LLM.\n */\nexport interface ParsedChatResponse {\n  /** Text explanation */\n  text: string;\n\n  /** Actionable suggestions */\n  suggestions: ChatSuggestion[];\n}\n\n/**\n * Generates a unique suggestion ID.\n */\nexport function generateSuggestionId(): string {\n  return `sug_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;\n}\n\n/**\n * Creates a summary from recommendations.\n */\nexport function createRecommendationSummary(\n  recommendations: SdrfRecommendation[]\n): RecommendationSummary {\n  const byType: Record<RecommendationType, number> = {\n    fill_value: 0,\n    correct_value: 0,\n    ontology_suggestion: 0,\n    consistency_fix: 0,\n    add_column: 0,\n  };\n\n  const byConfidence: Record<RecommendationConfidence, number> = {\n    high: 0,\n    medium: 0,\n    low: 0,\n  };\n\n  const affectedColumns = new Set<string>();\n  const affectedSamples = new Set<number>();\n\n  for (const rec of recommendations) {\n    byType[rec.type]++;\n    byConfidence[rec.confidence]++;\n    affectedColumns.add(rec.column);\n    for (const idx of rec.sampleIndices) {\n      affectedSamples.add(idx);\n    }\n  }\n\n  return {\n    total: recommendations.length,\n    byType,\n    byConfidence,\n    affectedColumns: Array.from(affectedColumns),\n    affectedSamples: affectedSamples.size,\n  };\n}\n", "/**\n * Base LLM Provider\n *\n * Abstract base class for LLM providers implementing common functionality:\n * - Fetch with timeout and abort controller\n * - Error handling and retry logic\n * - Streaming support infrastructure\n */\n\nimport {\n  LlmProviderType,\n  LlmProviderConfig,\n  LlmMessage,\n  LlmResponse,\n  LlmStreamChunk,\n  LlmError,\n  LlmErrorCode,\n  DEFAULT_PROVIDER_CONFIGS,\n} from '../../../models/llm';\n\n/**\n * Interface that all LLM providers must implement.\n */\nexport interface ILlmProvider {\n  /** Provider type identifier */\n  readonly name: LlmProviderType;\n\n  /** Whether this provider supports streaming */\n  readonly supportsStreaming: boolean;\n\n  /** Check if the provider is properly configured */\n  isConfigured(): boolean;\n\n  /** Get the current configuration */\n  getConfig(): LlmProviderConfig;\n\n  /** Update the configuration */\n  setConfig(config: Partial<LlmProviderConfig>): void;\n\n  /** Send messages and get a complete response */\n  complete(messages: LlmMessage[]): Promise<LlmResponse>;\n\n  /** Send messages and stream the response */\n  stream(messages: LlmMessage[]): AsyncGenerator<LlmStreamChunk, void, unknown>;\n\n  /** Abort any in-progress requests */\n  abort(): void;\n}\n\n/**\n * Abstract base class for LLM providers.\n */\nexport abstract class BaseLlmProvider implements ILlmProvider {\n  abstract readonly name: LlmProviderType;\n  abstract readonly supportsStreaming: boolean;\n\n  protected config: LlmProviderConfig;\n  protected abortController: AbortController | null = null;\n\n  constructor(config: Partial<LlmProviderConfig> = {}) {\n    // Get defaults for this provider type\n    const providerType = this.getProviderType();\n    const defaults = DEFAULT_PROVIDER_CONFIGS[providerType];\n\n    this.config = {\n      provider: providerType,\n      ...defaults,\n      ...config,\n    } as LlmProviderConfig;\n  }\n\n  /**\n   * Gets the provider type for this class.\n   * Subclasses should override this if needed during construction.\n   */\n  protected abstract getProviderType(): LlmProviderType;\n\n  /**\n   * Checks if the provider is configured with required credentials.\n   */\n  isConfigured(): boolean {\n    // Ollama doesn't require an API key\n    if (this.config.provider === 'ollama') {\n      return true;\n    }\n    return !!this.config.apiKey && this.config.apiKey.length > 0;\n  }\n\n  /**\n   * Gets the current configuration.\n   */\n  getConfig(): LlmProviderConfig {\n    return { ...this.config };\n  }\n\n  /**\n   * Updates the configuration.\n   */\n  setConfig(config: Partial<LlmProviderConfig>): void {\n    this.config = { ...this.config, ...config };\n  }\n\n  /**\n   * Aborts any in-progress requests.\n   */\n  abort(): void {\n    if (this.abortController) {\n      this.abortController.abort();\n      this.abortController = null;\n    }\n  }\n\n  /**\n   * Abstract method for sending a completion request.\n   */\n  abstract complete(messages: LlmMessage[]): Promise<LlmResponse>;\n\n  /**\n   * Abstract method for streaming a response.\n   */\n  abstract stream(messages: LlmMessage[]): AsyncGenerator<LlmStreamChunk, void, unknown>;\n\n  // ============ Protected Helper Methods ============\n\n  /**\n   * Makes a fetch request with timeout and abort handling.\n   */\n  protected async fetchWithTimeout(\n    url: string,\n    options: RequestInit\n  ): Promise<Response> {\n    // Create new abort controller for this request\n    this.abortController = new AbortController();\n\n    const timeoutId = setTimeout(() => {\n      this.abortController?.abort();\n    }, this.config.timeoutMs || 60000);\n\n    try {\n      const response = await fetch(url, {\n        ...options,\n        signal: this.abortController.signal,\n      });\n\n      return response;\n    } catch (error) {\n      if (error instanceof Error) {\n        if (error.name === 'AbortError') {\n          throw new LlmError(\n            'Request was aborted or timed out',\n            'TIMEOUT',\n            this.config.provider\n          );\n        }\n        throw new LlmError(\n          `Network error: ${error.message}`,\n          'NETWORK_ERROR',\n          this.config.provider,\n          error\n        );\n      }\n      throw error;\n    } finally {\n      clearTimeout(timeoutId);\n    }\n  }\n\n  /**\n   * Makes a streaming fetch request.\n   */\n  protected async fetchStream(\n    url: string,\n    options: RequestInit\n  ): Promise<ReadableStreamDefaultReader<Uint8Array>> {\n    const response = await this.fetchWithTimeout(url, options);\n\n    if (!response.ok) {\n      const errorBody = await response.text().catch(() => 'Unknown error');\n      throw this.createErrorFromResponse(response.status, errorBody);\n    }\n\n    if (!response.body) {\n      throw new LlmError(\n        'Response body is empty',\n        'PROVIDER_ERROR',\n        this.config.provider\n      );\n    }\n\n    return response.body.getReader();\n  }\n\n  /**\n   * Parses Server-Sent Events (SSE) from a stream.\n   */\n  protected async *parseSSEStream(\n    reader: ReadableStreamDefaultReader<Uint8Array>\n  ): AsyncGenerator<string, void, unknown> {\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done) {\n          break;\n        }\n\n        buffer += decoder.decode(value, { stream: true });\n\n        // Process complete lines\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          const trimmed = line.trim();\n\n          // Skip empty lines and comments\n          if (!trimmed || trimmed.startsWith(':')) {\n            continue;\n          }\n\n          // Parse data lines\n          if (trimmed.startsWith('data: ')) {\n            const data = trimmed.slice(6);\n\n            // Skip [DONE] marker\n            if (data === '[DONE]') {\n              return;\n            }\n\n            yield data;\n          }\n        }\n      }\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Creates an appropriate error from an HTTP response.\n   */\n  protected createErrorFromResponse(status: number, body: string): LlmError {\n    let code: LlmErrorCode = 'PROVIDER_ERROR';\n    let message = `API error (${status})`;\n\n    // Try to parse error details\n    try {\n      const parsed = JSON.parse(body);\n      message = parsed.error?.message || parsed.message || message;\n    } catch {\n      // Use raw body if not JSON\n      if (body.length < 200) {\n        message = body || message;\n      }\n    }\n\n    // Map status codes to error codes\n    switch (status) {\n      case 401:\n        code = 'INVALID_API_KEY';\n        message = 'Invalid API key. Please check your configuration.';\n        break;\n      case 429:\n        code = 'RATE_LIMITED';\n        message = 'Rate limit exceeded. Please wait before trying again.';\n        break;\n      case 408:\n      case 504:\n        code = 'TIMEOUT';\n        break;\n    }\n\n    return new LlmError(message, code, this.config.provider, { status, body });\n  }\n\n  /**\n   * Validates that the provider is configured before making requests.\n   */\n  protected validateConfiguration(): void {\n    if (!this.isConfigured()) {\n      throw new LlmError(\n        `${this.name} provider is not configured. Please provide an API key.`,\n        'NOT_CONFIGURED',\n        this.config.provider\n      );\n    }\n  }\n\n  /**\n   * Gets headers common to all requests.\n   */\n  protected getCommonHeaders(): Record<string, string> {\n    return {\n      'Content-Type': 'application/json',\n    };\n  }\n}\n\n/**\n * Utility to create a provider instance by type.\n */\nexport type ProviderFactory = (config?: Partial<LlmProviderConfig>) => ILlmProvider;\n"],
  "mappings": ";;;;;;;;;AA2CO,IAAM,2BAAgF;EAC3F,QAAQ;IACN,OAAO;IACP,SAAS;IACT,WAAW;IACX,WAAW;IACX,aAAa;;EAEf,WAAW;IACT,OAAO;IACP,SAAS;IACT,WAAW;IACX,WAAW;IACX,aAAa;;EAEf,QAAQ;IACN,OAAO;IACP,SAAS;IACT,WAAW;IACX,WAAW;IACX,aAAa;;EAEf,QAAQ;IACN,OAAO;IACP,SAAS;IACT,WAAW;IACX,WAAW;IACX,aAAa;;;AAOV,IAAM,mBAAsD;EACjE,QAAQ,CAAC,UAAU,eAAe,eAAe,eAAe;EAChE,WAAW,CAAC,4BAA4B,6BAA6B,wBAAwB;EAC7F,QAAQ,CAAC,oBAAoB,kBAAkB,kBAAkB;EACjE,QAAQ,CAAC,SAAS,YAAY,YAAY,WAAW,WAAW,aAAa,MAAM;;AAgP9E,IAAM,uBAAoC;EAC/C,gBAAgB;EAChB,WAAW,CAAA;EACX,gBAAgB;EAChB,aAAa;;AAQT,IAAO,WAAP,cAAwB,MAAK;EAGf;EACA;EACA;EAJlB,YACE,SACgB,MACA,UACA,SAAiB;AAEjC,UAAM,OAAO;AAJG,SAAA,OAAA;AACA,SAAA,WAAA;AACA,SAAA,UAAA;AAGhB,SAAK,OAAO;EACd;;AAqBI,SAAU,gCACd,UACA,OAAa;AAEb,SAAO;IACL,iBAAiB,CAAA;IACjB,SAAS;MACP,OAAO;MACP,QAAQ;QACN,YAAY;QACZ,eAAe;QACf,qBAAqB;QACrB,iBAAiB;QACjB,YAAY;;MAEd,cAAc;QACZ,MAAM;QACN,QAAQ;QACR,KAAK;;MAEP,iBAAiB,CAAA;MACjB,iBAAiB;;IAEnB,WAAW,oBAAI,KAAI;IACnB;IACA;;AAEJ;AAKM,SAAU,2BAAwB;AACtC,SAAO,OAAO,KAAK,IAAG,CAAE,IAAI,KAAK,OAAM,EAAG,SAAS,EAAE,EAAE,UAAU,GAAG,CAAC,CAAC;AACxE;AAKM,SAAU,uBAAuB,UAAyB;AAC9D,QAAM,QAAyC;IAC7C,QAAQ;IACR,WAAW;IACX,QAAQ;IACR,QAAQ;;AAEV,SAAO,MAAM,QAAQ;AACvB;AAKM,SAAU,uBAAuB,UAAyB;AAC9D,SAAO,aAAa;AACtB;AAKM,SAAU,qBAAqB,QAAqC;AACxE,MAAI,CAAC;AAAQ,WAAO;AACpB,MAAI,OAAO,aAAa;AAAU,WAAO;AACzC,SAAO,CAAC,CAAC,OAAO,UAAU,OAAO,OAAO,SAAS;AACnD;AAkFM,SAAU,uBAAoB;AAClC,SAAO,OAAO,KAAK,IAAG,CAAE,IAAI,KAAK,OAAM,EAAG,SAAS,EAAE,EAAE,UAAU,GAAG,CAAC,CAAC;AACxE;AAKM,SAAU,4BACd,iBAAqC;AAErC,QAAM,SAA6C;IACjD,YAAY;IACZ,eAAe;IACf,qBAAqB;IACrB,iBAAiB;IACjB,YAAY;;AAGd,QAAM,eAAyD;IAC7D,MAAM;IACN,QAAQ;IACR,KAAK;;AAGP,QAAM,kBAAkB,oBAAI,IAAG;AAC/B,QAAM,kBAAkB,oBAAI,IAAG;AAE/B,aAAW,OAAO,iBAAiB;AACjC,WAAO,IAAI,IAAI;AACf,iBAAa,IAAI,UAAU;AAC3B,oBAAgB,IAAI,IAAI,MAAM;AAC9B,eAAW,OAAO,IAAI,eAAe;AACnC,sBAAgB,IAAI,GAAG;IACzB;EACF;AAEA,SAAO;IACL,OAAO,gBAAgB;IACvB;IACA;IACA,iBAAiB,MAAM,KAAK,eAAe;IAC3C,iBAAiB,gBAAgB;;AAErC;;;ACnfM,IAAgB,kBAAhB,MAA+B;EAIzB;EACA,kBAA0C;EAEpD,YAAY,SAAqC,CAAA,GAAE;AAEjD,UAAM,eAAe,KAAK,gBAAe;AACzC,UAAM,WAAW,yBAAyB,YAAY;AAEtD,SAAK,SAAS;MACZ,UAAU;OACP,WACA;EAEP;;;;EAWA,eAAY;AAEV,QAAI,KAAK,OAAO,aAAa,UAAU;AACrC,aAAO;IACT;AACA,WAAO,CAAC,CAAC,KAAK,OAAO,UAAU,KAAK,OAAO,OAAO,SAAS;EAC7D;;;;EAKA,YAAS;AACP,WAAO,mBAAK,KAAK;EACnB;;;;EAKA,UAAU,QAAkC;AAC1C,SAAK,SAAS,kCAAK,KAAK,SAAW;EACrC;;;;EAKA,QAAK;AACH,QAAI,KAAK,iBAAiB;AACxB,WAAK,gBAAgB,MAAK;AAC1B,WAAK,kBAAkB;IACzB;EACF;;;;;EAiBgB,iBACd,KACA,SAAoB;;AAGpB,WAAK,kBAAkB,IAAI,gBAAe;AAE1C,YAAM,YAAY,WAAW,MAAK;AAChC,aAAK,iBAAiB,MAAK;MAC7B,GAAG,KAAK,OAAO,aAAa,GAAK;AAEjC,UAAI;AACF,cAAM,WAAW,MAAM,MAAM,KAAK,iCAC7B,UAD6B;UAEhC,QAAQ,KAAK,gBAAgB;UAC9B;AAED,eAAO;MACT,SAAS,OAAO;AACd,YAAI,iBAAiB,OAAO;AAC1B,cAAI,MAAM,SAAS,cAAc;AAC/B,kBAAM,IAAI,SACR,oCACA,WACA,KAAK,OAAO,QAAQ;UAExB;AACA,gBAAM,IAAI,SACR,kBAAkB,MAAM,OAAO,IAC/B,iBACA,KAAK,OAAO,UACZ,KAAK;QAET;AACA,cAAM;MACR;AACE,qBAAa,SAAS;MACxB;IACF;;;;;EAKgB,YACd,KACA,SAAoB;;AAEpB,YAAM,WAAW,MAAM,KAAK,iBAAiB,KAAK,OAAO;AAEzD,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,YAAY,MAAM,SAAS,KAAI,EAAG,MAAM,MAAM,eAAe;AACnE,cAAM,KAAK,wBAAwB,SAAS,QAAQ,SAAS;MAC/D;AAEA,UAAI,CAAC,SAAS,MAAM;AAClB,cAAM,IAAI,SACR,0BACA,kBACA,KAAK,OAAO,QAAQ;MAExB;AAEA,aAAO,SAAS,KAAK,UAAS;IAChC;;;;;EAKiB,eACf,QAA+C;;AAE/C,YAAM,UAAU,IAAI,YAAW;AAC/B,UAAI,SAAS;AAEb,UAAI;AACF,eAAO,MAAM;AACX,gBAAM,EAAE,MAAM,MAAK,IAAK,kBAAM,OAAO,KAAI;AAEzC,cAAI,MAAM;AACR;UACF;AAEA,oBAAU,QAAQ,OAAO,OAAO,EAAE,QAAQ,KAAI,CAAE;AAGhD,gBAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,mBAAS,MAAM,IAAG,KAAM;AAExB,qBAAW,QAAQ,OAAO;AACxB,kBAAM,UAAU,KAAK,KAAI;AAGzB,gBAAI,CAAC,WAAW,QAAQ,WAAW,GAAG,GAAG;AACvC;YACF;AAGA,gBAAI,QAAQ,WAAW,QAAQ,GAAG;AAChC,oBAAM,OAAO,QAAQ,MAAM,CAAC;AAG5B,kBAAI,SAAS,UAAU;AACrB;cACF;AAEA,oBAAM;YACR;UACF;QACF;MACF;AACE,eAAO,YAAW;MACpB;IACF;;;;;EAKU,wBAAwB,QAAgB,MAAY;AAC5D,QAAI,OAAqB;AACzB,QAAI,UAAU,cAAc,MAAM;AAGlC,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,IAAI;AAC9B,gBAAU,OAAO,OAAO,WAAW,OAAO,WAAW;IACvD,QAAQ;AAEN,UAAI,KAAK,SAAS,KAAK;AACrB,kBAAU,QAAQ;MACpB;IACF;AAGA,YAAQ,QAAQ;MACd,KAAK;AACH,eAAO;AACP,kBAAU;AACV;MACF,KAAK;AACH,eAAO;AACP,kBAAU;AACV;MACF,KAAK;MACL,KAAK;AACH,eAAO;AACP;IACJ;AAEA,WAAO,IAAI,SAAS,SAAS,MAAM,KAAK,OAAO,UAAU,EAAE,QAAQ,KAAI,CAAE;EAC3E;;;;EAKU,wBAAqB;AAC7B,QAAI,CAAC,KAAK,aAAY,GAAI;AACxB,YAAM,IAAI,SACR,GAAG,KAAK,IAAI,2DACZ,kBACA,KAAK,OAAO,QAAQ;IAExB;EACF;;;;EAKU,mBAAgB;AACxB,WAAO;MACL,gBAAgB;;EAEpB;;",
  "names": []
}
